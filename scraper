import re
from bs4 import BeautifulSoup
import sys
import warnings
from urllib import robotparser
import firebase_admin
from firebase_admin import credentials
from firebase_admin import db
from PIL import Image
import operator
from googlesearch import search
import json
import requests
import random
from urllib.request import Request, urlopen
from fake_useragent import UserAgent

ua = UserAgent()

def random_proxy():
  return random.randint(0, len(proxies) - 1)

proxies = []
proxies_req = Request('https://www.sslproxies.org/')
proxies_req.add_header('User-Agent', ua.random)
proxies_doc = urlopen(proxies_req).read().decode('utf8')

soup = BeautifulSoup(proxies_doc, 'html.parser')
proxies_table = soup.find(id='proxylisttable')

# Save proxies in the array
for row in proxies_table.tbody.find_all('tr'):
    proxies.append({
      'ip':   row.find_all('td')[0].string,
      'port': row.find_all('td')[1].string
    })

def shades_from_list_of_urls(list):
    shades_list = []
    size_dict = {}
    for src in list:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '//':
            src = src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src[-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'
        img_data = requests.get(src).content
        with open('aaa' + extens, 'wb') as handler:
            handler.write(img_data)

        try:
            im = Image.open('aaa' + extens)

            width, height = im.size
            if width < 101 and height < 101:

                if size_dict[(width, height)] == None:
                    size_dict[(width, height)] = 1
                else:
                    size_dict[(width, height)] = size_dict[(width, height)] + 1

            s = [(k, size_dict[k]) for k in sorted(size_dict, key=size_dict.get, reverse=True)]
            for k, v in s:
                often_size = k
                break

        except:
            continue

        for src in list:
            if src == '':
                continue
            if src[0] == '/' and src[1] == '//':
                src = src[2:]
            if src[0] == '/':
                src = prefix + website + postfix + src

            extens = src[-4:]
            if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
                continue
            if extens == 'jpeg':
                extens = '.jpeg'
            img_data = requests.get(src).content
            with open('aaa' + extens, 'wb') as handler:
                handler.write(img_data)

            try:
                im = Image.open('aaa' + extens)

                width, height = im.size

                if (width, height) == often_size:
                    print(im)
                    im = im.convert('RGB')
                    colors = im.getcolors(maxcolors=10000)
                    colors.sort(key=operator.itemgetter(0), reverse=True)
                    colors = colors[:3]
                    for i in range(0, len(colors) - 1):
                        if colors[i][1][0] + colors[i][1][1] + colors[i][1][2] >= 750 or colors[i][1][0] + colors[i][1][1] + \
                                colors[i][1][2] <= 15:
                            del colors[i]
                    if colors[0] != None:
                        shades_list.append(colors[0])
            except:
                continue
    return shades_list

#функция по превращению нзвания в стринги с тире и подчеркиваниями
def linkify(str):
    str = str.lower()
    output_list = []
    list_of_str = str.split()
    for start in range(0, len(list_of_str)):
        output_underscore = ''
        output_tire = ''
        output_no_space = ''
        for word in range(start, len(list_of_str)):
            if(word != start):
                output_underscore = output_underscore + "_" + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + "-" + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
            else:
                output_underscore = output_underscore + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
    if len(list_of_str) == 1:
        output_list.append(list_of_str[0])
    output_list.sort(key=len)
    output_list.reverse()
    return output_list

#подключаемся к бд
cred = credentials.Certificate('key.json')
firebase_admin.initialize_app(cred, {
    'databaseURL': 'https://makeapp-372a2.firebaseio.com/'
})

ref = db.reference()

file = open('brands.txt', 'r')
companies = file.readlines()

for company in companies:
    user_agent = ua.random

    website_full_name = ''
    company_name = ''
    company_name_with_extra = company.split()
    for i in range(0, len(company_name_with_extra) - 1):
        company_name = company_name + company_name_with_extra[i] + " "
    print("рассмотрим компанию " + company_name)

    for i in range(1, 11):
        proxy_index = random_proxy()
        proxy = proxies[proxy_index]
        try:
            pause = random.uniform(3, 5)
            potential_websites = search(company_name, tld="com", lang='en', num=3, stop=1, user_agent=user_agent, pause=pause)
        except:
            print("Skipping. Connnection error")
            continue

    for site in potential_websites:
        print(site)
        try:
            user_agent = ua.random
            headers = {'User-Agent': user_agent}
            data = requests.get(site, headers=headers)
        except:
            continue
        site_soup = BeautifulSoup(data.text, features='lxml')
        if site_soup.find(string=re.compile("brands")) == None and site_soup.find(string=re.compile("Brands")) == None and site_soup.find(string=re.compile("Shop by Brand")) == None:
            if site_soup.find(string="Lips") != None or site_soup.find(string="Eye") != None or site_soup.find(string="Eyes") != None or site_soup.find(string="Lipstick") != None:
                if(company_name[:-1].lower() in site_soup.find('title').text.lower()):
                    website_full_name = site
                websitified_name = company_name[:-1].lower()
                websitified_name = websitified_name.replace(" ", "")
                if websitified_name in site:
                    website_full_name = site
                print("сайт нашелся, это " + website_full_name)

                break
    if website_full_name == '':
        continue

    if website_full_name[:8] == 'https://':
        prefix = 'https://'
        prfx_len = 8
    else:
        prefix = 'http://'
        prfx_len = 7

    rest = website_full_name[prfx_len:]
    if rest[:4] == 'www.':
        rest = rest[4:]
    website = rest.split('.')[0]
    postfix = '.' + rest.split('.')[1].split('/')[0]

    print("префикс " + prefix)
    print("имя " + website)
    print("постфикс " + postfix)

    #добавляем компанию в бд
    if(ref.child('companies').order_by_child('name').equal_to(website).get() == None):
        ref.child('companies').push({'name' : website})

    #читаем роботс.тхт
    rp = robotparser.RobotFileParser()
    rp.set_url(prefix + website + postfix + "/robots.txt")
    try:
        rp.read()
    except:
        prefix = "http://"
        rp.set_url(prefix + website + postfix + "/robots.txt")
        rp.read()

    headers = {'User-Agent':user_agent}
    #пытаемся получить и прочесть карту сайта
    sitemap_url = ''
    robots = requests.get(prefix + website  + postfix + "/robots.txt", timeout = 10, headers = headers).text
    for line in robots.split("\n"):
        if line.startswith('Sitemap'):
            sitemap_url = line.split(': ')[1].split(' ')[0]

    if sitemap_url[0] == '/':
        sitemap_url = prefix + website + postfix + sitemap_url
    if sitemap_url != '':
        r = requests.get(sitemap_url, timeout = 10, headers = headers)
    else:
        r = requests.get(prefix + website  + postfix + "/sitemap.xml", timeout = 10, headers = headers)

    if not sys.warnoptions:
        warnings.simplefilter("ignore")
    xml = r.text
    sitemap_soup = BeautifulSoup(xml)

    sitemapindex = sitemap_soup.find('sitemapindex')

    if sitemapindex != None:
        sitemap_search = sitemap_soup.find('loc', text=re.compile('product'))
        if sitemap_search == None:
            sitemap_search = sitemap_soup.find('loc', text=re.compile('(en)(.*)(store|beauty|makeup)'))
        if sitemap_search == None:
            sitemap_search = sitemap_soup.find("loc")
        sitemap_url = sitemap_search.text
        r  = requests.get(sitemap_url, timeout = 10, headers = headers)
        data = r.text
        sitemap_soup = BeautifulSoup(data)

    internal_urls = []
    url_xmls = sitemap_soup.find_all("url")
    if url_xmls != []:
        for url in url_xmls:
            internal_urls.append(url.findNext("loc").text)
    if url_xmls == []:
        url_xmls = sitemap_soup.find_all("a")
        for url in url_xmls:
            internal_urls.append(url.text)
    if url_xmls == []:
        r = requests.get(sitemap_url, timeout = 10, headers = headers)
        data = r.text
        messy_urls = data.split()
        for messy_url in messy_urls:
            internal_urls.append(re.split('[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])T(2[0-3]|[01][0-9]):[0-10][0-9]Z')[0])

    all_items=[]

    #если карты сайта не оказалось
    if(internal_urls == [] or sitemap_url == ''):
        print("карты сайта нет, сделаем сами")
        visited_urls = []
        for_visiting_urls = [prefix + website  + postfix]
        while for_visiting_urls != []:
            try:
                page = requests.get(for_visiting_urls[0], timeout = 10, headers = headers)
                if for_visiting_urls[0] not in visited_urls:
                    visited_urls.append(for_visiting_urls[0])
                soup = BeautifulSoup(page.content, 'html.parser')
                potential_urls = soup.find_all('a')
                for url in potential_urls:
                    if url.has_attr('href') and prefix+website+postfix in url['href']:
                        if (url['href'] not in visited_urls and url['href'] not in for_visiting_urls):
                            if rp.can_fetch("*", url['href']) and url['href'] != '':
                                if url['href'][0] != '#':
                                    if url['href'][0] == '/':
                                        url['href'] = prefix + website  + postfix + url['href']
                                    if prefix+website+postfix in url['href']:
                                        for_visiting_urls.append(url['href'])
                del for_visiting_urls[0]
            except:
                del for_visiting_urls[0]
        internal_urls = visited_urls

    for url in internal_urls:
        item_array=[]
        try:
            page = requests.get(url, timeout = 10, headers = headers)
        except:
            continue
        soup = BeautifulSoup(page.content, 'html.parser')

        #парсим цену
        price_elements = soup.find_all("span", {"class": re.compile("price|money")})

        if price_elements == []:
            price_elements = soup.find_all("p", {"class": re.compile("price")})
        if price_elements == []:
            price_elements = soup.find_all("div", {"class": re.compile("price")})
        if price_elements != []:
            prices = [e for e in price_elements if not e.find_parents(attrs={"class": re.compile("header")})]
            if(prices != []):
                if len(prices) < 5:
                    price = prices[0].text
                    print(price)
                    price_flg = True
                else:
                    price_flg = False
                    print('слишком много цен на странице')
        else:
            price_flg = False
            print("с этой страницы цена не распарсилась")

        page_title = soup.find("title")

        name = None
        #парсим название
        name_elements = soup.find_all(itemprop={re.compile("name")})
        name_elements = name_elements + soup.find_all(id={re.compile("name")})
        name_elements = name_elements + soup.find_all(class_=re.compile("name"))
        name_elements = name_elements + soup.find_all(class_=re.compile("title"))
        name_elements = name_elements + soup.find_all("h1")
        name_elements = name_elements + soup.find_all("h2")
        if name_elements != []:
            names = [e for e in name_elements if not e.find_parents(attrs={"class": re.compile("header")}) and not e.name == 'meta']
            if(names != []):
                if len(names) < 20:
                    frequency_map = {}
                    for name in names:
                        if name.string != None and name.string in page_title.string:
                            if name.string.replace(" ", "") != "":
                                frequency_map[len(soup.find_all(string=re.compile(name.string)))] = name.string
                    keys = frequency_map.keys()
                    if frequency_map == {}:
                        name = None
                    for key in sorted(frequency_map, reverse=True):
                        name = frequency_map[key]
                        if '&' in name or '#' in name:
                            name = json.dumps(name)
                        print(name)
                        break
                else:
                    name = None
                    print("слишком много названий, заголовок: " + page_title.string)

        if name == None:
            name_flg = False
            print("с этой страницы название не распарсилось")
        else:
            name_flg = True

        #парсим картинки
        pics = []
        images = soup.find_all("img", alt=True)
        if name != None:
            for pic in images:
                if(names != []):
                    if name != None:
                        if name in pic['alt']:
                            pics.append(pic['src'])

        if pics == [] and name != None:
            linkified_names = linkify(name)
            for link_name in linkified_names:
                link_name = re.escape(link_name)
                found = soup.find_all(src=re.compile(link_name))
                if found != []:
                    for pic in found:
                        if pic['src'] not in pics:
                            pics.append(pic['src'])

        if pics == [] and name != None:
            linkified_names = linkify(name)
            for link_name in linkified_names:
                link_name = re.escape(link_name)
                found = soup.find_all(srcset=re.compile(link_name))
                if found != []:
                    for pic in found:
                        if pic['src'] not in pics:
                            pics.append(pic['src'])

        if pics == []:
            imgs = soup.find_all('img')
            for img in imgs:
                pics.append(img['src'])

        if pics == []:
            pic_flg = False
            print("с этой страницы картинки не распарсились")
        else:
            pic_flg = True
            if len(pics) > 5:
                for src in pics:
                    if src == '':
                        continue
                    if src[0] == '/' and src[1] == '//':
                        src = src[2:]
                    if src[0] == '/':
                        src = prefix + website + postfix + src

                    extens = src[-4:]
                    if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
                        continue
                    if extens == 'jpeg':
                        extens = '.jpeg'
                    img_data = requests.get(src).content
                    with open('aaa' + extens, 'wb') as handler:
                        handler.write(img_data)

                    try:
                        im = Image.open('aaa' + extens)

                        width, height = im.size
                        if width < 101 and height < 101:
                            pics.remove(src)
                    except:
                        pics.remove(src)
            print(pics)

        shades_flg=False
        #оттенки
        if pic_flg and name_flg and price_flg:
            shades = []
            img_tag = soup.find_all('img')
            imgs = []
            for el in img_tag:
                if el.has_attr('src'):
                    imgs.append(el['src'])
            shades = shades_from_list_of_urls(imgs)

            if shades == []:
                results = soup.findAll(True, {"background-color": re.compile('#')})
                additional = soup.findAll(True, {"style": re.compile('background-color:#')})
                for el in additional:
                    hex = re.match('(?<=background-color:#).{6}', el['style'])
                    if hex != None:
                        shades.append(hex)
                for res in results:
                    if res['background-color'] != None:
                        shades.append(res['background-color'])

            if shades == []:
                els = soup.find_all('a', {'data-url':True})
                for el in els:
                    shades.append(el['data-url'])
            if shades != []:
                shades_flg = True
                print(shades)
            else:
                shades_flg = False
                print("с этой страницы оттенки не распарсились")


        #добавляем продукт в бд
        if pic_flg and name_flg and price_flg and shades_flg:
            if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(website).push({'name': name, 'price':price, 'pics': pics, 'shades': shades})
        elif pic_flg and name_flg and shades_flg:
            if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(website).push({'name': name, 'pics': pics, 'shades': shades})
        elif pic_flg and name_flg:
            if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(website).push({'name': name, 'pics': pics})