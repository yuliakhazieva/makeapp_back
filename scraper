import re
from bs4 import BeautifulSoup
import sys
import warnings
from urllib import robotparser
import firebase_admin
from firebase_admin import credentials
from firebase_admin import db
from PIL import Image
import operator
from googlesearch import search
import requests
import random
from fake_useragent import UserAgent
import signal
import unidecode

def process_names(name_elements):
    name = None

    if name_elements == []:
        return

    names = [e for e in name_elements if not e.find_parents(attrs={"class": re.compile("header")}) and not e.name == 'meta']

    if (names != []):
        frequency_map = {}

        for name in names:

            name_temp = None

            if name.string != None:
                name_temp = name.string
            if name.text != None:
                name_temp = name.text

            if name_temp != None:
                name_temp = name_temp.strip(" ")
                name_temp = ''.join(name_temp.splitlines())

                if page_title.string != None:
                    if name_temp in page_title.string.strip(" ") and name_temp != '':
                        frequency_map[len(soup.find_all(string=re.compile(name_temp)))] = name_temp

                if name_temp != None and name_temp != '':
                    linkified_names = linkify(name_temp)

                    for link_name in linkified_names:
                        if link_name in url:
                            if name_temp != None:
                                nms = soup.find_all(True, string=re.compile(re.escape(name_temp)))
                                frequency_map[len(nms)] = name_temp
                                break
        if frequency_map == {}:
            name = None
        for key in sorted(frequency_map, reverse=True):
            name = frequency_map[key]
            if '&' in name or '#' in name:
                name = name.replace('&', "and")
                name = name.replace('#', "")
            break
    if name != None:
        name = name.strip(" ")
        name = ''.join(name.splitlines())
    return name

def handler(signum, frame):
        print ("Forever is over!")
        raise Exception("end of time")

keywords = ['eyeshadow', 'lipstick', 'lipgloss', 'lip liner', 'highlighter',
                        'bronzer', 'foundation', 'concealer', 'mascara', 'eyeliner', 'blush',
                        'primer', 'powder', 'contouring', 'eye pencil', 'pigment', 'glitter',
                        'bb creme', 'cc creme', 'brow', 'Eyeshadow', 'Lipstick', 'Lipgloss',
                        'Lip liner', 'Highlighter', 'Bronzer', 'Foundation', 'Concealer',
                        'Mascara', 'Eyeliner', 'Blush','Primer', 'Powder', 'Contouring',
                        'Eye pencil', 'Pigment', 'Glitter', 'BB creme', 'CC creme', 'Brow',
                        'LIPS', 'Lips', 'lips', 'EYES', 'Eyes', 'eyes', 'Тени', "тени",
                        "Помада", "помада", "Помады", "помады", "Блеск", "блеск", "Блески",
                        "блески", "Карандаш для губ", "карандаш для губ"]

def keywords_in_soup(site_soup):
    for word in keywords:
        if site_soup.find(string=re.compile(word)) != None:
            return True
    return False


def delete_small_pics(pics):
    pics_with_size = []
    for src in pics:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '/':
            src = prefix + src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src.split('?')[0][-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'

        try:
            img_data = requests.get(src).content
        except:
            src_els = src.split('://')
            src = src_els[0]+ '://www.' + src_els[1]
            img_data = requests.get(src).content

        with open('aaa' + extens, 'wb') as handler:
            handler.write(img_data)

        try:
            im = Image.open('aaa' + extens)

            width, height = im.size
            pics_with_size.append((src, width, height))
        except:
            if src in pics:
                pics.remove(src)

    w = 600
    h = 600
    pics = [x[0] for x in pics_with_size if x[1] > w and x[2] > h]
    while pics == [] and w > 300:
        w = w - 100
        h = h - 100
        pics = [x[0] for x in pics_with_size if x[1] > w and x[2] > h and 'background' not in src]
    return pics


def shades_from_list_of_urls(list):
    shades_list = []
    size_dict = {}
    often_size = None
    for src in list:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '//':
            src = src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src[-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'

        try:
            img_data = requests.get(src).content
            with open('aaa' + extens, 'wb') as handler:
                handler.write(img_data)

            try:
                im = Image.open('aaa' + extens)

                width, height = im.size
                if width < 101 and height < 101:

                    if size_dict[(width, height)] == None:
                        size_dict[(width, height)] = 1
                    else:
                        size_dict[(width, height)] = size_dict[(width, height)] + 1

                s = [(k, size_dict[k]) for k in sorted(size_dict, key=size_dict.get, reverse=True)]
                for k, v in s:
                    often_size = k
                    break

            except:
                continue
        except:
            continue

    for src in list:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '//':
            src = src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src[-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'
        img_data = requests.get(src).content
        with open('aaa' + extens, 'wb') as handler:
            handler.write(img_data)

        try:
            im = Image.open('aaa' + extens)

            width, height = im.size
            if often_size != None:
                if (width, height) == often_size:
                    print(im)
                    im = im.convert('RGB')
                    colors = im.getcolors(maxcolors=10000)
                    colors.sort(key=operator.itemgetter(0), reverse=True)
                    colors = colors[:3]
                    for i in range(0, len(colors) - 1):
                        if colors[i][1][0] + colors[i][1][1] + colors[i][1][2] >= 750 \
                                or colors[i][1][0] + colors[i][1][1] + colors[i][1][2] <= 15:
                            del colors[i]
                    if colors[0] != None:
                        shades_list.append(colors[0])
        except:
            continue
    return shades_list

#функция по превращению нзвания в стринги с тире и подчеркиваниями
def linkify(str):
    str = str.lower()
    output_list = []
    list_of_str = str.split()
    for start in range(0, len(list_of_str)):
        output_underscore = ''
        output_tire = ''
        output_no_space = ''
        for word in range(start, len(list_of_str)):
            if(word != start):
                output_underscore = output_underscore + "_" + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + "-" + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
            else:
                output_underscore = output_underscore + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
    if len(list_of_str) == 1:
        output_list.append(list_of_str[0])
    output_list.sort(key=len)
    output_list.reverse()
    return output_list


ua = UserAgent()


#подключаемся к бд
cred = credentials.Certificate('key.json')
firebase_admin.initialize_app(cred, {
    'databaseURL': 'https://makeapp-372a2.firebaseio.com/'
})

ref = db.reference()

file = open('brands.txt', 'r')
companies = file.readlines()

for company in companies:
    user_agent = ua.random

    website_full_name = ''
    company_name = ''
    company_name_with_extra = company.split()
    for i in range(0, len(company_name_with_extra) - 1):
        company_name = company_name + company_name_with_extra[i] + " "
    print("рассмотрим компанию " + company_name)

    try:
        pause = random.uniform(3, 5)
        potential_websites = search(company_name, tld="com", lang='en', num=3, stop=1, user_agent=user_agent, pause=pause)
    except:
        print("Skipping. Connnection error")
        continue

    for site in potential_websites:
        print(site)
        try:
            user_agent = ua.random
            headers = {'User-Agent': user_agent}
            data = requests.get(site, headers=headers)
        except:
            continue

        site_soup = BeautifulSoup(data.text, features='lxml')

        if site_soup.find(string="brands") == None and site_soup.find(string="Brands") == None \
                and site_soup.find(string="Shop by Brand") == None and site_soup.find(string="BRANDS") == None:

            if site_soup.find(string="makeup") != None \
                    or site_soup.find(string="Makeup") != None \
                    or site_soup.find(string="MAKE UP") != None \
                    or site_soup.find(string="Cosmetics") != None \
                    or site_soup.find(string="cosmetics") != None \
                    or site_soup.find(string="Make Up") != None \
                    or site_soup.find('title') != None and ('Makeup' in site_soup.find('title').text
                                                            or 'Cosmetics' in site_soup.find('title').text
                                                            or 'Make up' in site_soup.find('title').text
                                                            or 'Maquillage' in site_soup.find('title').text) \
                    or keywords_in_soup(site_soup):


                websitified_name = company_name[:-1].lower()
                websitified_name = websitified_name.replace(" ", "")
                websitified_name = websitified_name.replace(".", "")
                if websitified_name in site or site_soup.find('title') != None and (company_name[:-1].lower() in site_soup.find('title').text.lower()):
                    website_full_name = site
                    print("сайт нашелся, это " + website_full_name)
                    break

                company_name = unidecode.unidecode(company_name)
                websitified_name = company_name[:-1].lower()
                websitified_name = websitified_name.replace(" ", "")
                websitified_name = websitified_name.replace(".", "")
                if websitified_name in site or site_soup.find('title') != None and (
                        company_name[:-1].lower() in site_soup.find('title').text.lower()):
                    website_full_name = site
                    print("сайт нашелся, это " + website_full_name)
                    break

    if website_full_name == '':
        continue

    if website_full_name[:8] == 'https://':
        prefix = 'https://'
        prfx_len = 8
    else:
        prefix = 'http://'
        prfx_len = 7

    rest = website_full_name[prfx_len:]
    if rest[:4] == 'www.':
        rest = rest[4:]
    website = ''
    for i in range(0, len(rest.split('.')) - 1):
        if website == '':
            website = website + rest.split('.')[i]
        else:
            website = website + "." + rest.split('.')[i]

    postfix = rest.split(website)[1].split('/')[0]
    if postfix != '.com' and postfix != '.biz' and postfix != '.ru':
        continue

    if website == 'amazon' or website == 'twitter' or re.match('yahoo', website) or website == 'youtube':
        continue

    try:
        data = requests.get(prefix + website + postfix, headers=headers)
    except:
        try:
            data = requests.get(prefix + 'www.' + website + postfix, headers=headers)
        except:
            continue
    site_soup = BeautifulSoup(data.text, features='lxml')
    if not (site_soup.find(string="brands") == None and site_soup.find(string="Brands") == None \
            and site_soup.find(string="Shop by Brand") == None and site_soup.find(string="BRANDS") == None):

        if not (site_soup.find(string="makeup") != None \
                or site_soup.find(string="Makeup") != None \
                or site_soup.find(string="MAKE UP") != None \
                or site_soup.find(string="Cosmetics") != None \
                or site_soup.find(string="cosmetics") != None \
                or site_soup.find(string="Make Up") != None \
                or site_soup.find('title') != None and ('Makeup' in site_soup.find('title').text
                                                        or 'Cosmetics' in site_soup.find('title').text
                                                        or 'Make up' in site_soup.find('title').text
                                                        or 'Maquillage' in site_soup.find('title').text) \
                or keywords_in_soup(site_soup)):
            continue
    websitified_name = company_name[:-1].lower()
    websitified_name = websitified_name.replace(" ", "")
    if websitified_name not in prefix+website+postfix and company_name[:-1].lower() not in site_soup.find('title').text.lower():
        continue



    print("префикс " + prefix)
    print("имя " + website)
    print("постфикс " + postfix)

    #добавляем компанию в бд
    if(ref.child('companies').order_by_child('name').equal_to(company_name).get() == None):
        ref.child('companies').push({'name' : company_name})

    #читаем роботс.тхт
    rp = robotparser.RobotFileParser()
    rp.set_url(prefix + website + postfix + "/robots.txt")
    try:
        rp.read()
    except:
        print("робот.тхт не прочитался")

    headers = {'User-Agent':user_agent}
    #пытаемся получить и прочесть карту сайта
    sitemap_url = ''
    try:
        robots = requests.get(prefix + website  + postfix + "/robots.txt", timeout = 10, headers = headers).text
        for line in robots.split("\n"):
            if line.startswith('Sitemap'):
                sitemap_url = line.split(': ')[1].split(' ')[0]
    except:
        print("робот.тхт не прочитался")
    if sitemap_url != '':
        if sitemap_url[0] == '/':
            sitemap_url = prefix + website + postfix + sitemap_url

    r = None
    if sitemap_url != '':
        try:
            r = requests.get(sitemap_url, timeout = 10, headers = headers)
        except:
            sitemap_url = ''
    if sitemap_url == '':
        sitemap_url = prefix + website  + postfix + "/sitemap.xml"
        try:
            r = requests.get(prefix + website  + postfix + "/sitemap.xml", timeout = 10, headers = headers)
        except:
            try:
                r = requests.get(prefix + 'www.' + website + postfix + "/sitemap.xml", timeout=10, headers=headers)
            except:
                sitemap_url = ''

    if not sys.warnoptions:
        warnings.simplefilter("ignore")

    internal_urls = []
    if r != None:
        xml = r.text
        sitemap_soup = BeautifulSoup(xml)
        sitemapindex = sitemap_soup.find('sitemapindex')
        if sitemapindex != None:
            sitemap_search = sitemap_soup.find('loc', text=re.compile('product'))
            if sitemap_search == None:
                sitemap_search = sitemap_soup.find('loc', text=re.compile('(en)(.*)(store|beauty|makeup)'))
            if sitemap_search == None:
                sitemap_search = sitemap_soup.find("loc")
            sitemap_url = sitemap_search.text
            r  = requests.get(sitemap_url, timeout = 10, headers = headers)
            data = r.text
            sitemap_soup = BeautifulSoup(data)
        internal_urls = []
        url_xmls = sitemap_soup.find_all("url")
        if url_xmls != []:
            for url in url_xmls:
                internal_urls.append(url.findNext("loc").text)
        if url_xmls == []:
            url_xmls = sitemap_soup.find_all("a")
            for url in url_xmls:
                internal_urls.append(url.text)
        all_items=[]
        signal.signal(signal.SIGALRM, handler)
        signal.alarm(300)

    #если карты сайта не оказалось
    try:
        if(internal_urls == [] or sitemap_url == '' or len(internal_urls) < 10):
            internal_urls = ref.child('companies').child(company_name).child('sitemap').get()['map']
            print("карты сайта нет, сделаем сами")
            visited_urls = []
            for_visiting_urls = [prefix + 'www.' + website  + postfix]
            while for_visiting_urls != []:
                try:
                    page = requests.get(for_visiting_urls[0], timeout = 10, headers = headers)
                    if for_visiting_urls[0] not in visited_urls:
                        visited_urls.append(for_visiting_urls[0])
                        print("посещаю для карты " + for_visiting_urls[0])
                        soup = BeautifulSoup(page.content, 'html.parser')
                        potential_urls = soup.find_all('a')
                        for url in potential_urls:
                            if url.has_attr('href') and (prefix+ 'www.' + website+postfix in url['href'] or prefix+website+postfix in url['href'] or url['href'][0] == '/'):
                                if (url['href'] not in visited_urls and url['href'] not in for_visiting_urls
                                        and prefix + website  + postfix + url['href'] not in for_visiting_urls
                                        and prefix + website  + postfix + url['href'] not in visited_urls):
                                    if url['href'] != '':
                                        if url['href'][0] != '#':
                                            if url['href'][0] == '/':
                                                url['href'] = prefix + website  + postfix + url['href']
                                            if prefix+website+postfix in url['href']:
                                                for_visiting_urls.append(url['href'])
                    del for_visiting_urls[0]
                except:
                    del for_visiting_urls[0]
            internal_urls = visited_urls
            ref.child('companies').child(company_name).child('sitemap').push({'map': visited_urls})
    except:
        internal_urls = []

    signal.alarm(0)



    for url in internal_urls:
        print("--------------------------------------------------------")
        print("looking at " + url)
        item_array=[]
        try:
            page = requests.get(url, timeout = 10, headers = headers)
        except:
            continue
        soup = BeautifulSoup(page.content, 'html.parser')






        #парсим цену
        price_elements = soup.find_all(True, {'class': re.compile("price")})
        price_elements = price_elements + soup.find_all(True, {'class': re.compile("money")})
        price_elements = price_elements + soup.find_all(_class=re.compile("Price"))
        price_elements = price_elements + soup.find_all(_class=re.compile("msrp"))
        price_elements = price_elements + soup.find_all(_class=re.compile("MSRP"))
        price_elements = price_elements + soup.find_all(itemprop= re.compile("price|Price"))
        if price_elements != []:

            prices = [e for e in price_elements if e.text != ''
                      and not e.find_parents(attrs={"class": re.compile("header")})
                      and not e.find_parents(attrs={"class": re.compile("cart")})]
            if(prices != []):
                price = None
                for pr in prices:
                    temp = pr.text
                    temp.replace(" ", "")
                    temp = ''.join(temp.splitlines())
                    if temp != "" and '.' in temp:
                        price = temp
                        break
                if price != None:
                    print(price)
                    price_flg = True
                else:
                    price_flg = False
                    print("с этой страницы цена не распарсилась")
            else:
                for pr in price_elements:
                    temp = pr.text
                    temp.replace(" ", "")
                    temp = ''.join(temp.splitlines())
                    if temp != "" and '.' in temp:
                        price = temp
                        break
                if price != None:
                    print(price)
                    price_flg = True
                else:
                    price_flg = False
                    print("с этой страницы цена не распарсилась")
        else:
            price_flg = False
            print("с этой страницы цена не распарсилась")








        page_title = soup.find("title")
        name = None
        #парсим название
        name_elements = soup.find_all(True, itemprop={re.compile("name")})
        name = process_names(name_elements)
        if name  == '' or name == None:
            name_elements = soup.find_all(id={re.compile("name")})
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = name_elements + soup.find_all("h1")
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = soup.find_all(class_=re.compile("name"))
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = soup.find_all(class_=re.compile("title"))
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = name_elements + soup.find_all("h2")
            name = process_names(name_elements)

        if name == None or name == '':
            name_flg = False
            print("с этой страницы название не распарсилось")
        else:
            print(name)
            name_flg = True






        #парсим картинки
        if name_flg:
            imgs = soup.find_all("img", alt=re.compile(name))
            for img in imgs:
                if img.has_attr('src'):
                    pics.append(img['src'])
            pics = delete_small_pics(pics)

            if pics == [] and name != None:
                linkified_names = linkify(name)
                for link_name in linkified_names:
                    link_name = re.escape(link_name)
                    found = soup.find_all(src=re.compile(link_name))
                    if found != []:
                        for pic in found:
                            if pic['src'] not in pics:
                                pics.append(pic['src'])

                pics = delete_small_pics(pics)

            if pics == [] and name != None:
                linkified_names = linkify(name)
                for link_name in linkified_names:
                    link_name = re.escape(link_name)
                    found = soup.find_all(srcset=re.compile(link_name))
                    if found != []:
                        for pic in found:
                            if pic['srcset'] not in pics:
                                pics.append(pic['srcset'])

                pics = delete_small_pics(pics)

            if pics == []:
                imgs = soup.find_all('img')
                for img in imgs:
                    if img.has_attr('src'):
                        pics.append(img['src'])

                pics = delete_small_pics(pics)

            if pics == []:
                pic_flg = False
                print("с этой страницы картинки не распарсились")
            else:
                print(pics)
                pic_flg = True

        else:
            pic_flg = False
            print("нет смысла смотреть картинки")



        if name_flg:
            #описание
            descriptions = soup.find_all(_class = re.compile('description'))
            if descriptions == []:
                descriptions = soup.find_all(id=re.compile('description'))
            if descriptions == []:
                descriptions = soup.find_all('p')
            if descriptions != []:
                descriptions.sort(key = lambda s: len(s.text), reverse=True)
                desc = descriptions[0].string
                print(desc)
                desc_flg = True
            else:
                desc_flg = False



        if name_flg:
            #категория
            category = ''
            for keyword in keywords:
                if keyword in name:
                    category = keyword
                    break
            if category == '':
                if desc != None:
                    for keyword in keywords:
                        if keyword in desc:
                            category = keyword
                            break
            if category == '':
                category = 'other'
            print(category)



        shades_flg=False
        #оттенки
        if pic_flg and name_flg and price_flg:
            shades = []
            img_tag = soup.find_all('img')
            imgs = []
            for el in img_tag:
                if el.has_attr('src'):
                    imgs.append(el['src'])
            shades = shades_from_list_of_urls(imgs)

            if shades == []:
                results = soup.findAll(True, {"background-color": re.compile('#')})
                additional = soup.findAll(True, {"style": re.compile('background-color:#')})
                for el in additional:
                    hex = re.match('(?<=background-color:#).{6}', el['style'])
                    if hex != None:
                        shades.append(hex)
                for res in results:
                    if res['background-color'] != None:
                        shades.append(res['background-color'])

            if shades == []:
                els = soup.find_all('a', {'data-url':True})
                for el in els:
                    shades.append(el['data-url'])
            if shades != []:
                shades_flg = True
                print(shades)
            else:
                shades_flg = False
                print("с этой страницы оттенки не распарсились")






        #добавляем продукт в бд
        if pic_flg and name_flg and price_flg and shades_flg and desc_flg:
            if (len(ref.child('companies').child(company_name).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(company_name).child(category).push({'name': name, 'price':price, 'pics': pics, 'shades': shades, 'description': desc})
        elif pic_flg and name_flg and price_flg and shades_flg:
            if (len(ref.child('companies').child(company_name).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(company_name).child(category).push({'name': name, 'pics': pics, 'shades': shades, 'description': desc})
        elif pic_flg and name_flg and price_flg:
            if (len(ref.child('companies').child(company_name).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(company_name).child(category).push({'name': name, 'pics': pics})
        elif pic_flg and name_flg and shades_flg and desc_flg:
            if (len(ref.child('companies').child(company_name).order_by_child('name').equal_to(name).get()) == 0):
                ref.child('companies').child(company_name).child(category).push({'name': name, 'pics': pics, 'shades': shades, 'description': desc})