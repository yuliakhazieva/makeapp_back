import re
from bs4 import BeautifulSoup
import sys
import warnings
import requests
from urllib import robotparser
import firebase_admin
from firebase_admin import credentials
from firebase_admin import db

cred = credentials.Certificate('key.json')
firebase_admin.initialize_app(cred, {
    'databaseURL': 'https://makeapp-372a2.firebaseio.com/'
})

ref = db.reference()

prefix = "https://"
website = "kyliecosmetics"
postfix = '.com'

if(ref.child('companies').order_by_child('name').equal_to(website).get() == None):
    ref.child('companies').push({'name' : website})

rp = robotparser.RobotFileParser()
rp.set_url(prefix + website + postfix + "/robots.txt")
try:
    rp.read()
except:
    prefix = "http://"
    rp.set_url(prefix + website + postfix + "/robots.txt")
    rp.read()

sitemap_url = ''
robots = requests.get(prefix + website  + postfix + "/robots.txt").text
for line in robots.split("\n"):
    if line.startswith('Sitemap'):
        sitemap_url = line.split(': ')[1].split(' ')[0]

if sitemap_url != '':
    r = requests.get(sitemap_url)
else:
    r = requests.get(prefix + website  + postfix + "/sitemap.xml")

if not sys.warnoptions:
    warnings.simplefilter("ignore")
xml = r.text
sitemap_soup = BeautifulSoup(xml)

sitemapindex = sitemap_soup.find('sitemapindex')

if sitemapindex != None:
    sitemap_search = sitemap_soup.find('loc', text=re.compile('product'))
    if sitemap_search == None:
        sitemap_search = sitemap_soup.find('loc', text=re.compile('(en)(.*)(store|beauty|makeup)'))
    sitemap_url = sitemap_search.text
    r  = requests.get(sitemap_url)
    data = r.text
    sitemap_soup = BeautifulSoup(data)

internal_urls = []
url_xmls = sitemap_soup.find_all("url")
if url_xmls != []:
    for url in url_xmls:
        internal_urls.append(url.findNext("loc").text)
if url_xmls == []:
    url_xmls = sitemap_soup.find_all("a")
    for url in url_xmls:
        internal_urls.append(url.text)
if url_xmls == []:
    r = requests.get(sitemap_url)
    data = r.text
    messy_urls = data.split()
    for messy_url in messy_urls:
        internal_urls.append(re.split('[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])T(2[0-3]|[01][0-9]):[0-5][0-9]Z')[0])


headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
    }

def linkify(str):
    str = str.lower()
    output_list = []
    list_of_str = str.split()
    for start in range(0, len(list_of_str) - 1):
        output_underscore = ''
        output_tire = ''
        for word in range(start, len(list_of_str) - 2):
            output_underscore = output_underscore + list_of_str[word] + "_"
            output_list.append(output_underscore)
            output_tire = output_tire + list_of_str[word] + "-"
            output_list.append(output_tire)
    if len(list_of_str) == 1:
        output_list.append(list_of_str[0])
    output_list.sort(key=len)
    output_list.reverse()
    return output_list

all_items=[]

if(internal_urls == []):
    visited_urls = []
    for_visiting_urls = [prefix + website  + postfix]
    while for_visiting_urls != []:
        page = requests.get(for_visiting_urls[0])
        visited_urls.append(for_visiting_urls[0])
        del for_visiting_urls[0]
        soup = BeautifulSoup(page.content, 'html.parser')
        potential_urls = soup.find_all('a')
        for url in potential_urls:
            if url.has_attr('href'):
                if (url['href'] not in visited_urls and url['href'] not in for_visiting_urls):
                    if rp.can_fetch("*", url['href']) and url['href'] != '':
                        if url['href'][0] != '#':
                            if url['href'][0] == '/':
                                url['href'] = prefix + website  + postfix + url['href']
                            for_visiting_urls.append(url['href'])
    internal_urls = visited_urls

for url in internal_urls:
    item_array=[]
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')

    price = soup.find_all("span", {"class": re.compile("price|money")})

    if price == []:
        price = soup.find_all("p", {"class": re.compile("price")})
    if price == None:
        price = soup.find_all("div", {"class": re.compile("price")})
    if price != None:
        price_correct = [e for e in price if not e.find_parents(attrs={"class": re.compile("header")})]
        if(price_correct != []):
            print(price_correct[0].text)
            price_flg = True
    else:
        price_flg = False

    name = soup.find_all(itemprop={re.compile("name")})
    if name == []:
        name = soup.find_all(id={re.compile("name")})
    if name == []:
        name = soup.find_all(class_=re.compile("name"))
    if name == []:
        name = soup.find_all(class_=re.compile("title"))
    if name != []:
        name_correct = [e for e in name if not e.find_parents(attrs={"class": re.compile("header")}) and not e.name == 'meta']
        if(name_correct != []):
            print(name_correct[0].string)
            name_flg = True
    else:
        name_flg = False

    pics = []
    images = soup.find_all("img", alt=True)
    if name != None:
        for pic in images:
            if(name_correct != []):
                if name_correct[0].string != None:
                    if name_correct[0].string in pic['alt']:
                        pics.append(pic['src'])

    if pics == [] and name_correct != []:
        linkified_names = linkify(name_correct[0].text)
        for link_name in linkified_names:
            found = soup.find_all(src=re.compile(link_name))
            if found != []:
                if found not in pics:
                    pics = pics + found
        for pic in pics:
            pic = pic['src']
        print('image none')

    if pics == [] and name_correct != []:
        linkified_names = linkify(name_correct[0].text)
        for link_name in linkified_names:
            found = soup.find_all(srcset=re.compile(link_name))
            if found != []:
                if found not in pics:
                    pics = pics + found
        for pic in pics:
            pic = pic['src']

    if pics == []:
        pic_flg = False
    else:
        pic_flg = True

    if pic_flg and name_flg:
        if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name_correct[0].text).get()) == 0):
            ref.child('companies').child(website).push({'name': name_correct[0].string, 'price':price_correct[0].string, 'pics': pics})