import re
from bs4 import BeautifulSoup
import sys
import warnings
from urllib import robotparser
import firebase_admin
from firebase_admin import credentials
from firebase_admin import db
from PIL import Image
import requests
import time
import operator

def shades_from_list_of_urls(list):
    for src in list:
        if src[0] == '/' and src[1] == '//':
            src = src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src[-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'
        img_data = requests.get(src).content
        with open('aaa' + extens, 'wb') as handler:
            handler.write(img_data)

        im = Image.open('aaa' + extens)

        width, height = im.size
        if width < 101 and height < 101:
            print(im)
            im = im.convert('RGB')
            colors = im.getcolors(maxcolors=10000)
            colors.sort(key=operator.itemgetter(0), reverse=True)
            colors = colors[:3]
            for i in range(0, len(colors) - 1):
                if colors[i][1][0] + colors[i][1][1] + colors[i][1][2] >= 750 or colors[i][1][0] + colors[i][1][1] + \
                        colors[i][1][2] <= 15:
                    del colors[i]
            shades.append(colors[0])

def jasonify(str):
    list = str.split('&')
    output = ''
    for el in range(0, len(list) - 1):
        output = output + list[el] + 'and'
    output = output + list[len(list)-1]
    return output

#функция по превращению нзвания в стринги с тире и подчеркиваниями
def linkify(str):
    str = str.lower()
    output_list = []
    list_of_str = str.split()
    for start in range(0, len(list_of_str)):
        output_underscore = ''
        output_tire = ''
        output_no_space = ''
        for word in range(start, len(list_of_str)):
            if(word != start):
                output_underscore = output_underscore + "_" + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + "-" + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
            else:
                output_underscore = output_underscore + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
    if len(list_of_str) == 1:
        output_list.append(list_of_str[0])
    output_list.sort(key=len)
    output_list.reverse()
    return output_list

#подключаемся к бд
cred = credentials.Certificate('key.json')
firebase_admin.initialize_app(cred, {
    'databaseURL': 'https://makeapp-372a2.firebaseio.com/'
})

ref = db.reference()

#источник данных
prefix = "https://"
website = "urbandecay"
postfix = '.ru'

#добавляем компанию в бд
if(ref.child('companies').order_by_child('name').equal_to(website).get() == None):
    ref.child('companies').push({'name' : website})

headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:13.0) Gecko/20100101 Firefox/13.0.1'
    }


#читаем роботс.тхт
rp = robotparser.RobotFileParser()
rp.set_url(prefix + website + postfix + "/robots.txt")
try:
    rp.read()
except:
    prefix = "http://"
    rp.set_url(prefix + website + postfix + "/robots.txt")
    rp.read()

#пытаемся получить и прочесть карту сайта
sitemap_url = ''
robots = requests.get(prefix + website  + postfix + "/robots.txt", timeout = 10, headers = headers).text
for line in robots.split("\n"):
    if line.startswith('Sitemap'):
        sitemap_url = line.split(': ')[1].split(' ')[0]

if sitemap_url != '':
    r = requests.get(sitemap_url, timeout = 10, headers = headers)
else:
    r = requests.get(prefix + website  + postfix + "/sitemap.xml", timeout = 10, headers = headers)

if not sys.warnoptions:
    warnings.simplefilter("ignore")
xml = r.text
sitemap_soup = BeautifulSoup(xml)

sitemapindex = sitemap_soup.find('sitemapindex')

if sitemapindex != None:
    sitemap_search = sitemap_soup.find('loc', text=re.compile('product'))
    if sitemap_search == None:
        sitemap_search = sitemap_soup.find('loc', text=re.compile('(en)(.*)(store|beauty|makeup)'))
    if sitemap_search == None:
        sitemap_search = sitemap_soup.find("loc")
    sitemap_url = sitemap_search.text
    r  = requests.get(sitemap_url, timeout = 10, headers = headers)
    data = r.text
    sitemap_soup = BeautifulSoup(data)

internal_urls = []
url_xmls = sitemap_soup.find_all("url")
if url_xmls != []:
    for url in url_xmls:
        internal_urls.append(url.findNext("loc").text)
if url_xmls == []:
    url_xmls = sitemap_soup.find_all("a")
    for url in url_xmls:
        internal_urls.append(url.text)
if url_xmls == []:
    r = requests.get(sitemap_url, timeout = 10, headers = headers)
    data = r.text
    messy_urls = data.split()
    for messy_url in messy_urls:
        internal_urls.append(re.split('[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1])T(2[0-3]|[01][0-9]):[0-10][0-9]Z')[0])

all_items=[]

#если карты сайта не оказалось
if(internal_urls == [] or sitemap_url == ''):
    visited_urls = []
    for_visiting_urls = [prefix + website  + postfix]
    while for_visiting_urls != []:
        page = requests.get(for_visiting_urls[0], timeout = 10, headers = headers)
        visited_urls.append(for_visiting_urls[0])
        del for_visiting_urls[0]
        soup = BeautifulSoup(page.content, 'html.parser')
        potential_urls = soup.find_all('a')
        for url in potential_urls:
            if url.has_attr('href'):
                if (url['href'] not in visited_urls and url['href'] not in for_visiting_urls):
                    if rp.can_fetch("*", url['href']) and url['href'] != '':
                        if url['href'][0] != '#':
                            if url['href'][0] == '/':
                                url['href'] = prefix + website  + postfix + url['href']
                            for_visiting_urls.append(url['href'])
    internal_urls = visited_urls

for url in internal_urls:
    item_array=[]
    page = requests.get(url, timeout = 10, headers = headers)
    soup = BeautifulSoup(page.content, 'html.parser')

    #парсим цену
    price_elements = soup.find_all("span", {"class": re.compile("price|money")})

    if price_elements == []:
        price_elements = soup.find_all("p", {"class": re.compile("price")})
    if price_elements == []:
        price_elements = soup.find_all("div", {"class": re.compile("price")})
    if price_elements != []:
        prices = [e for e in price_elements if not e.find_parents(attrs={"class": re.compile("header")})]
        if(prices != []):
            price = prices[0].text
            print(price)
            price_flg = True
    else:
        price_flg = False

    page_title = soup.find("title")

    name = None
    #парсим название
    name_elements = soup.find_all(itemprop={re.compile("name")})
    if name_elements == []:
        name_elements = soup.find_all(id={re.compile("name")})
    if name_elements == []:
        name_elements = soup.find_all(class_=re.compile("name"))
    if name_elements == []:
        name_elements = soup.find_all(class_=re.compile("title"))
    if name_elements != []:
        names = [e for e in name_elements if not e.find_parents(attrs={"class": re.compile("header")}) and not e.name == 'meta']
        if(names != []):
            frequency_map = {}
            for name in names:
                if name.string != None and name.string in page_title.string:
                    frequency_map[len(soup.find_all(string=re.compile(name.string)))] = name.string
            keys = frequency_map.keys()
            if frequency_map == {}:
                name = None
            for key in sorted(frequency_map, reverse=True):
                name = frequency_map[key]
                if '&' in name:
                    name = jasonify(name)
                print(name)
                break
            name_flg = True
    else:
        name_flg = False

    #парсим картинки
    pics = []
    images = soup.find_all("img", alt=True)
    if name != None:
        for pic in images:
            if(names != []):
                if name != None:
                    if name in pic['alt']:
                        pics.append(pic['src'])

    if pics == [] and name != None:
        linkified_names = linkify(name)
        for link_name in linkified_names:
            found = soup.find_all(src=re.compile(link_name))
            if found != []:
                for pic in found:
                    if pic['src'] not in pics:
                        pics.append(pic['src'])

    if pics == [] and name != None:
        linkified_names = linkify(name)
        for link_name in linkified_names:
            found = soup.find_all(srcset=re.compile(link_name))
            if found != []:
                for pic in found:
                    if pic['src'] not in pics:
                        pics.append(pic['src'])

    if pics == []:
        pic_flg = False
    else:
        pic_flg = True
        print(pics)


    #оттенки
    if pic_flg and name_flg:
        shades = []
        img_tag = soup.find_all('img')
        imgs = []
        for el in img_tag:
            imgs.append(el['src'])
        shades_from_list_of_urls(imgs)

        if shades == []:
            results = soup.findAll(True, {"background-color": re.compile('#')})
            additional = soup.findAll(True, {"style": re.compile('background-color:#')})
            for el in additional:
                hex = re.match('(?<=background-color:#).{6}', el['style'])
                shades.append(hex)
            for res in results:
                shades.append(res['background-color'])

        if shades == []:
            els = soup.find_all('a', {'data-url':True})
            for el in els:
                shades.append(el['data-url'])
        if shades != []:
            shades_flg = True
        else:
            shades_flg = False


    #добавляем продукт в бд
    if pic_flg and name_flg and price_flg and shades_flg:
        if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name).get()) == 0):
            ref.child('companies').child(website).push({'name': name, 'price':price, 'pics': pics, 'shades': shades})
    elif pic_flg and name_flg and shades_flg:
        if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name).get()) == 0):
            ref.child('companies').child(website).push({'name': name, 'pics': pics, 'shades': shades})
    elif pic_flg and name_flg:
        if (len(ref.child('companies').child(website).order_by_child('name').equal_to(name).get()) == 0):
            ref.child('companies').child(website).push({'name': name, 'pics': pics})