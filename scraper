import re
from bs4 import BeautifulSoup
import warnings
import firebase_admin
from firebase_admin import credentials
from firebase_admin import db
import operator
from googlesearch import search
import requests
import random
from fake_useragent import UserAgent
import unidecode
import numpy as np
import sys
import tensorflow as tf
from object_detection.utils import label_map_util
import urllib.parse
from distutils.version import StrictVersion
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.12.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.12.*.')

MODEL_NAME = 'inference_graph'
PATH_TO_FROZEN_GRAPH ='object_detection/inference_graph_new/frozen_inference_graph.pb'
PATH_TO_LABELS = '/Users/a1/Desktop/object-detection.pbtxt'

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)

def get_elize():
    try:
        pageelize = requests.get('https://elize.ru/search/' + urllib.parse.quote_plus(name), timeout=10)
    except:
        return None
    elize_soup = BeautifulSoup(pageelize.content, 'html.parser')
    pr = elize_soup.find('span', {'class':'price'})
    if(pr == None):
        return None
    return pr.text


def get_ildebote():
    try:
        pageilde = requests.get('https://iledebeaute.ru/shop/search?q=' + urllib.parse.quote_plus(name), timeout=10, headers=headers)
    except:
        return None
    ilde_soup = BeautifulSoup(pageilde.content, 'html.parser')
    if(ilde_soup.find(string = 'ничего не найдено.') != None):
        return None
    if(ilde_soup.find_all('a', {'title': re.compile(name)}) != []):
        return ilde_soup.find_all("span", {"class": "def"})[24].text
    return None

def get_goldenapple():
    try:
        pageilde = requests.get('https://goldapple.ru/catalogsearch/result/?q=' + urllib.parse.quote_plus(name), timeout=10, headers=headers)
    except:
        return None
    ilde_soup = BeautifulSoup(pageilde.content, 'html.parser')
    if (ilde_soup.find("span", {"class": "catalog-product-name-span"}) != name):
        return None
    return ilde_soup.find("span", {"class": "price"}).text


def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)

def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[1], image.shape[2])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: image})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict




def process_names(name_elements):
    name = None

    if name_elements == []:
        return

    names = [e for e in name_elements if not e.find_parents(attrs={"class": re.compile("header")}) and not e.name == 'meta']

    if (names != []):
        frequency_map = {}

        for name in names:

            name_temp = None

            if name.string != None:
                name_temp = name.string
            if name.text != None:
                name_temp = name.text

            if name_temp != None:
                name_temp = name_temp.strip(" ")
                name_temp = ''.join(name_temp.splitlines())

                if page_title != None:
                    if page_title.string != None:
                        if name_temp in page_title.string.strip(" ") and name_temp != '':
                            frequency_map[len(soup.find_all(string=re.compile(name_temp)))] = name_temp

                if name_temp != None and name_temp != '':
                    linkified_names = linkify(name_temp)

                    for link_name in linkified_names:
                        if link_name in url:
                            if name_temp != None:
                                nms = soup.find_all(True, string=re.compile(re.escape(name_temp)))
                                frequency_map[len(nms)] = name_temp
                                break
        if frequency_map == {}:
            name = None
        for key in sorted(frequency_map, reverse=True):
            name = frequency_map[key]
            if '&' in name or '#' in name:
                name = name.replace('&', "and")
                name = name.replace('#', "")
            break
    if name != None:
        name = name.strip(" ")
        name = ''.join(name.splitlines())
    return name

def handler(signum, frame):
        print ("Forever is over!")
        raise Exception("end of time")

keywords = ['eyeshadow', 'lipstick', 'lipgloss', 'lip liner', 'highlighter',
                        'bronzer', 'foundation', 'concealer', 'mascara', 'eyeliner', 'blush',
                        'primer', 'powder', 'contouring', 'eye pencil', 'pigment', 'glitter',
                        'bb creme', 'cc creme', 'brow', 'Eyeshadow', 'Lipstick', 'Lipgloss',
                        'Lip liner', 'Highlighter', 'Bronzer', 'Foundation', 'Concealer',
                        'Mascara', 'Eyeliner', 'Blush','Primer', 'Powder', 'Contouring',
                        'Eye pencil', 'Pigment', 'Glitter', 'BB creme', 'CC creme', 'Brow',
                        'LIPS', 'Lips', 'lips', 'EYES', 'Eyes', 'eyes', 'lip', 'LIP', 'Lip', 'Тени', "тени",
                        "Помада", "помада", "Помады", "помады", "Блеск", "блеск", "Блески",
                        "блески", "Карандаш для губ", "карандаш для губ"]

def keywords_in_soup(site_soup):
    for word in keywords:
        if site_soup.find(string=re.compile(word)) != None:
            return True
    return False


def delete_small_pics(pics):
    pics_with_size = []
    for src in pics:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '/':
            src = prefix + src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src.split('?')[0][-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'

        try:
            img_data = requests.get(src).content
        except:
            if src[:4] == 'http':
                src_els = src.split('://')
                src = src_els[0]+ '://www.' + src_els[1]
                try:
                    img_data = requests.get(src).content
                except:
                    print("oops")
            else:
                src = prefix + website + postfix + '/' + src
                try:
                    img_data = requests.get(src).content
                except:
                    continue

        try:
            with open('aaa' + extens, 'wb') as handler:
                handler.write(img_data)

            try:
                im = Image.open('aaa' + extens)

                width, height = im.size
                pics_with_size.append((src, width, height))
            except:
                if src in pics:
                    pics.remove(src)
        except:
            print('idk man')

    w = 600
    h = 600
    pics = [x[0] for x in pics_with_size if x[1] > w and x[2] > h]
    while pics == [] and w > 300:
        w = w - 100
        h = h - 100
        pics = [x[0] for x in pics_with_size if x[1] > w and x[2] > h and 'background' not in src]
    return pics


def shades_from_list_of_urls(list):
    shades_list = []
    size_dict = {}
    often_size = None
    for src in list:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '//':
            src = src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src[-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'

        try:
            img_data = requests.get(src).content
            with open('aaa' + extens, 'wb') as handler:
                handler.write(img_data)

            try:
                im = Image.open('aaa' + extens)

                width, height = im.size
                if width < 101 and height < 101:

                    if size_dict[(width, height)] == None:
                        size_dict[(width, height)] = 1
                    else:
                        size_dict[(width, height)] = size_dict[(width, height)] + 1

                s = [(k, size_dict[k]) for k in sorted(size_dict, key=size_dict.get, reverse=True)]
                for k, v in s:
                    often_size = k
                    break

            except:
                continue
        except:
            continue

    for src in list:
        if src == '':
            continue
        if src[0] == '/' and src[1] == '//':
            src = src[2:]
        if src[0] == '/':
            src = prefix + website + postfix + src

        extens = src[-4:]
        if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
            continue
        if extens == 'jpeg':
            extens = '.jpeg'
        try:
            img_data = requests.get(src).content
        except:
            if src[:4] == 'http':
                src_els = src.split('://')
                src = src_els[0]+ '://www.' + src_els[1]
                img_data = requests.get(src).content
            else:
                src = prefix + website + postfix + '/' + src
                img_data = requests.get(src).content
        with open('aaa' + extens, 'wb') as handler:
            handler.write(img_data)

        try:
            im = Image.open('aaa' + extens)

            width, height = im.size
            if often_size != None:
                if (width, height) == often_size:
                    print(im)
                    im = im.convert('RGB')
                    colors = im.getcolors(maxcolors=10000)
                    colors.sort(key=operator.itemgetter(0), reverse=True)
                    colors = colors[:3]
                    for i in range(0, len(colors) - 1):
                        if colors[i][1][0] + colors[i][1][1] + colors[i][1][2] >= 750 \
                                or colors[i][1][0] + colors[i][1][1] + colors[i][1][2] <= 15:
                            del colors[i]
                    if colors[0] != None:
                        shades_list.append(colors[0])
        except:
            continue
    return shades_list

#функция по превращению нзвания в стринги с тире и подчеркиваниями
def linkify(str):
    str = str.lower()
    output_list = []
    list_of_str = str.split()
    for start in range(0, len(list_of_str)):
        output_underscore = ''
        output_tire = ''
        output_no_space = ''
        for word in range(start, len(list_of_str)):
            if(word != start):
                output_underscore = output_underscore + "_" + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + "-" + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
            else:
                output_underscore = output_underscore + list_of_str[word]
                output_list.append(output_underscore)
                output_tire = output_tire + list_of_str[word]
                output_list.append(output_tire)
                output_no_space = output_no_space + list_of_str[word]
                output_list.append(output_no_space)
    if len(list_of_str) == 1:
        output_list.append(list_of_str[0])
    output_list.sort(key=len)
    output_list.reverse()
    return output_list


ua = UserAgent()


#подключаемся к бд
cred = credentials.Certificate('key.json')
firebase_admin.initialize_app(cred, {
    'databaseURL': 'https://makeapp-372a2.firebaseio.com/'
})

ref = db.reference()

file = open('brands.txt', 'r')
companies = file.readlines()

for company in companies:
    user_agent = ua.random

    website_full_name = ''
    company_name = ''
    company_name_with_extra = company.split()
    for i in range(0, len(company_name_with_extra) - 1):
        company_name = company_name + company_name_with_extra[i] + " "
    print("рассмотрим компанию " + company_name)

    try:
        pause = random.uniform(3, 5)
        potential_websites = search(company_name, tld="com", lang='en', num=3, stop=1, user_agent=user_agent, pause=pause)
    except:
        print("Skipping. Connnection error")
        continue

    for site in potential_websites:
        print(site)
        try:
            user_agent = ua.random
            headers = {'User-Agent': user_agent}
            data = requests.get(site, headers=headers)
        except:
            continue

        website_full_name = site
        if website_full_name[:8] == 'https://':
            prefix = 'https://'
            prfx_len = 8
        else:
            prefix = 'http://'
            prfx_len = 7

        rest = website_full_name[prfx_len:]
        if rest[:4] == 'www.':
            rest = rest[4:]
        website = ''
        for i in range(0, len(rest.split('.')) - 1):
            if website == '':
                website = website + rest.split('.')[i]
            else:
                website = website + "." + rest.split('.')[i]

        postfix = rest.split(website)[1].split('/')[0]
        if postfix != '.com' and postfix != '.biz' and postfix != '.ru' and postfix != '.us':
            continue

        if website == 'amazon' or website == 'twitter' or website == 'podrygka' or re.match('yahoo', website) or website == 'youtube':
            continue

        site_soup = BeautifulSoup(data.text, features='lxml')

        if site_soup.find(string="brands") == None and site_soup.find(string="Brands") == None \
                and site_soup.find(string="Shop by Brand") == None and site_soup.find(string="BRANDS") == None:

            if site_soup.find(string="makeup") != None \
                    or site_soup.find(string="Makeup") != None \
                    or site_soup.find(string="MAKE UP") != None \
                    or site_soup.find(string="Cosmetics") != None \
                    or site_soup.find(string="cosmetics") != None \
                    or site_soup.find(string="Make Up") != None \
                    or site_soup.find('title') != None and ('Makeup' in site_soup.find('title').text
                                                            or 'Cosmetics' in site_soup.find('title').text
                                                            or 'Make up' in site_soup.find('title').text
                                                            or 'Maquillage' in site_soup.find('title').text) \
                    or keywords_in_soup(site_soup):


                websitified_name = company_name[:-1].lower()
                websitified_name = websitified_name.replace(" ", "")
                websitified_name = websitified_name.replace(".", "")
                if websitified_name in site or site_soup.find('title') != None and (company_name[:-1].lower() in site_soup.find('title').text.lower()):
                    website_full_name = site
                    print("сайт нашелся, это " + website_full_name)
                    break

                company_name = unidecode.unidecode(company_name)
                websitified_name = company_name[:-1].lower()
                websitified_name = websitified_name.replace(" ", "")
                websitified_name = websitified_name.replace(".", "")
                if websitified_name in site or site_soup.find('title') != None and (
                        company_name[:-1].lower() in site_soup.find('title').text.lower()):
                    website_full_name = site
                    print("сайт нашелся, это " + website_full_name)
                    break

    if website_full_name == '':
        continue

    if website_full_name[:8] == 'https://':
        prefix = 'https://'
        prfx_len = 8
    else:
        prefix = 'http://'
        prfx_len = 7

    rest = website_full_name[prfx_len:]
    if rest[:4] == 'www.':
        rest = rest[4:]
    website = ''
    for i in range(0, len(rest.split('.')) - 1):
        if website == '':
            website = website + rest.split('.')[i]
        else:
            website = website + "." + rest.split('.')[i]

    postfix = rest.split(website)[1].split('/')[0]
    if postfix != '.com' and postfix != '.biz' and postfix != '.ru' and postfix != '.us':
        continue

    if website == 'amazon' or website == 'twitter' or re.match('yahoo', website) or website == 'youtube':
        continue

    try:
        data = requests.get(prefix + website + postfix, headers=headers)
    except:
        try:
            data = requests.get(prefix + 'www.' + website + postfix, headers=headers)
        except:
            continue
    site_soup = BeautifulSoup(data.text, features='lxml')
    if not (site_soup.find(string="brands") == None and site_soup.find(string="Brands") == None \
            and site_soup.find(string="Shop by Brand") == None and site_soup.find(string="BRANDS") == None):

        if not (site_soup.find(string="makeup") != None \
                or site_soup.find(string="Makeup") != None \
                or site_soup.find(string="MAKE UP") != None \
                or site_soup.find(string="Cosmetics") != None \
                or site_soup.find(string="cosmetics") != None \
                or site_soup.find(string="Make Up") != None \
                or site_soup.find('title') != None and ('Makeup' in site_soup.find('title').text
                                                        or 'Cosmetics' in site_soup.find('title').text
                                                        or 'Make up' in site_soup.find('title').text
                                                        or 'Maquillage' in site_soup.find('title').text) \
                or keywords_in_soup(site_soup)):
            continue
    websitified_name = company_name[:-1].lower()
    websitified_name = websitified_name.replace(" ", "")
    if websitified_name not in prefix+website+postfix and company_name[:-1].lower() not in site_soup.find('title').text.lower():
        continue

    #добавляем компанию в бд
    if(ref.child('companies').order_by_child('name').equal_to(company_name).get() == None):
        ref.child('companies').push({'name' : company_name})

    # #читаем роботс.тхт
    # rp = robotparser.RobotFileParser()
    # rp.set_url(prefix + website + postfix + "/robots.txt")
    # try:
    #     rp.read()
    # except:
    #     print("робот.тхт не прочитался")

    headers = {'User-Agent':user_agent}
    #пытаемся получить и прочесть карту сайта
    sitemap_url = ''
    try:
        robots = requests.get(prefix + website  + postfix + "/robots.txt", timeout = 10, headers = headers).text
        for line in robots.split("\n"):
            if line.startswith('Sitemap'):
                sitemap_url = line.split(': ')[1].split(' ')[0]
                break
    except:
        try:
            rbt_url = prefix + 'www.' + website + postfix + "/robots.txt"
            robots = requests.get(rbt_url, timeout=10, headers=headers).text
            for line in robots.split("\n"):
                if line.startswith('Sitemap') or 'sitemap' in line:
                    try:
                        if line.startswith('Sitemap'):
                            sitemap_url = line.split(': ')[1].split(' ')[0]
                        else:
                            sitemap_url = line
                    except:
                        continue
                    break
        except:
            print("робот.тхт не прочитался")
    if sitemap_url != '':
        if sitemap_url[0] == '/':
            sitemap_url = prefix + website + postfix + sitemap_url

    r = None
    if sitemap_url != '':
        try:
            if(not sitemap_url.endswith('l')):
                sitemap_url = sitemap_url[:-1]
            r = requests.get(sitemap_url, timeout = 10, headers = headers)
        except:
            sitemap_url = ''
    if sitemap_url == '':
        sitemap_url = prefix + website  + postfix + "/sitemap.xml"
        try:
            if (not sitemap_url.endswith('l')):
                sitemap_url = sitemap_url.sub[:-1]
            r = requests.get(prefix + website  + postfix + "/sitemap.xml", timeout = 10, headers = headers)
        except:
            try:
                if (not sitemap_url.endswith('l')):
                    sitemap_url = sitemap_url.sub[:-1]
                r = requests.get(prefix + 'www.' + website + postfix + "/sitemap.xml", timeout=10, headers=headers)
            except:
                sitemap_url = ''

    if not sys.warnoptions:
        warnings.simplefilter("ignore")

    internal_urls = []
    if r != None:
        xml = r.text
        sitemap_soup = BeautifulSoup(xml)
        sitemapindex = sitemap_soup.find('sitemapindex')
        if sitemapindex != None:
            sitemap_search = sitemap_soup.find('loc', text=re.compile('product'))
            if sitemap_search == None:
                sitemap_search = sitemap_soup.find('loc', text=re.compile('(us)(.*)(store|beauty|makeup)?'))
            if sitemap_search == None:
                sitemap_search = sitemap_soup.find("loc")
            sitemap_url = sitemap_search.text
            try:
                r  = requests.get(sitemap_url, timeout = 10, headers = headers)
            except:
                continue
            data = r.text
            sitemap_soup = BeautifulSoup(data)
        internal_urls = []
        url_xmls = sitemap_soup.find_all("url")
        if url_xmls != []:
            for url in url_xmls:
                internal_urls.append(url.findNext("loc").text)
        if url_xmls == []:
            url_xmls = sitemap_soup.find_all("a")
            for url in url_xmls:
                internal_urls.append(url.text)
        all_items=[]

    #если карты сайта не оказалось
    try:
        if(internal_urls == [] or sitemap_url == '' or len(internal_urls) < 50 or internal_urls[0][:4] != 'http'):
            print("карты сайта нет, сделаем сами")
            visited_urls = []
            for_visiting_urls = [prefix + 'www.' + website  + postfix]
            while for_visiting_urls != []:
                try:
                    page = requests.get(for_visiting_urls[0], timeout = 10, headers = headers)
                    if for_visiting_urls[0] not in visited_urls:
                        visited_urls.append(for_visiting_urls[0])
                        print("посещаю для карты " + for_visiting_urls[0])
                        soup = BeautifulSoup(page.content, 'html.parser')
                        potential_urls = soup.find_all('a')
                        for url in potential_urls:
                            if url.has_attr('href') and url['href'] != '' and (prefix+ 'www.' + website in url['href'] or prefix+website in url['href'] or url['href'][0] == '/'):
                                if (url['href'] not in visited_urls and url['href'] not in for_visiting_urls
                                        and prefix + website  + postfix + url['href'] not in for_visiting_urls
                                        and prefix + website  + postfix + url['href'] not in visited_urls):
                                    if url['href'] != '':
                                        if url['href'][0] != '#':
                                            if url['href'][0] == '/':
                                                url['href'] = prefix + website  + postfix + url['href']
                                            if website+postfix in url['href']:
                                                if (len(url['href'].split('/')[3]) == 2 or re.match('..-..', url['href'].split('/')[3]) != None) and 'int' in url['href'].split('/')[3].lower() or 'us' in url['href'].split('/')[3].lower() or (len(url['href'].split('/')[3]) != 2 and re.match('..-..', url['href'].split('/')[3]) == None):
                                                    for_visiting_urls.append(url['href'])
                    del for_visiting_urls[0]
                except:
                    del for_visiting_urls[0]
            internal_urls = visited_urls
            ref.child('companies').child(company_name).child('sitemap').push({'map': visited_urls})
    except Exception as error:
        print(error)




    for url in internal_urls:

        if(len(url.split('/')) > 3):
            if((len(url.split('/')[3]) == 2 or re.match('..(-|_)..', url.split('/')[3])) and 'us' not in url.split('/')[3] and 'uk' not in url.split('/')[3]):
                continue

        print("--------------------------------------------------------")
        print("looking at " + url)
        item_array=[]
        try:
            page = requests.get(url, timeout = 10, headers = headers)
        except:
            continue
        soup = BeautifulSoup(page.content, 'html.parser')

        #парсим цену
        price = None
        price_elements = soup.find_all(True, {'class': re.compile("price")})
        price_elements = price_elements + soup.find_all(True, {'class': re.compile("money")})
        price_elements = price_elements + soup.find_all(_class=re.compile("Price"))
        price_elements = price_elements + soup.find_all(_class=re.compile("msrp"))
        price_elements = price_elements + soup.find_all(_class=re.compile("MSRP"))
        price_elements = price_elements + soup.find_all(itemprop= re.compile("price|Price"))
        if price_elements != []:

            prices = [e for e in price_elements if e.text != ''
                      and not e.find_parents(attrs={"class": re.compile("header")})
                      and not e.find_parents(attrs={"class": re.compile("cart")})]
            prices.sort(key=len)
            if(prices != []):
                for pr in prices:
                    temp = pr.text
                    temp.replace(" ", "")
                    temp = ''.join(temp.splitlines())
                    if temp != "" and bool(re.search(r'\d', temp)):
                        price = re.sub("[^0-9|.]", "", temp)
                        break
                if price != None:
                    price_flg = True
                else:
                    price_flg = False
                    print("с этой страницы цена не распарсилась")
            else:
                for pr in price_elements:
                    temp = pr.text
                    temp.replace(" ", "")
                    temp = ''.join(temp.splitlines())
                    if temp != "" and bool(re.search(r'\d', temp)):
                        price = re.sub("[^0-9|.]", "", temp)
                        break
                if price != None:
                    price_flg = True
                else:
                    price_flg = False
                    print("с этой страницы цена не распарсилась")
        else:
            price_flg = False
            print("с этой страницы цена не распарсилась")
        if price != None:
            pic_flg = True
            filter(lambda x: x.isdigit() or x == ',', price)
            if (price[-2:] == "00"):
                price = price[:-2] + ',' + price[-2:]

            print(price)

            # price = re.sub("\D", "", price)




        page_title = soup.find("title")
        #парсим название
        name_elements = soup.find_all(True, itemprop={re.compile("name")})
        name = process_names(name_elements)
        if name  == '' or name == None:
            name_elements = soup.find_all(id={re.compile("name")})
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = name_elements + soup.find_all("h1")
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = soup.find_all(class_=re.compile("name"))
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = soup.find_all(class_=re.compile("title"))
            name = process_names(name_elements)
        if name == '' or name == None:
            name_elements = name_elements + soup.find_all("h2")
            name = process_names(name_elements)

        if name == None or name == '':
            name_flg = False
            print("с этой страницы название не распарсилось")
        else:
            print(name)
            name_flg = True

        import time
        #парсим картинки
        if name_flg:
            imgs = soup.find_all("img", alt=re.compile(name))
            pics = []
            for img in imgs:
                # if img.has_attr('src'):
                #     pics.append(img['src'])
                for attr in img.attrs.values():
                    if type(attr).__name__ != 'list':
                        if '.jpg' in attr or '.jpeg' in attr or '.png' in attr:
                            pics.append(attr)
            pics = delete_small_pics(pics)

            if pics == [] and name != None:
                linkified_names = linkify(name)
                for link_name in linkified_names:
                    link_name = re.escape(link_name)
                    found = soup.find_all(src=re.compile(link_name))
                    if found != []:
                        for pic in found:
                            if pic['src'] not in pics:
                                pics.append(pic['src'])

                pics = delete_small_pics(pics)

            if pics == [] and name != None:
                linkified_names = linkify(name)
                for link_name in linkified_names:
                    link_name = re.escape(link_name)
                    found = soup.find_all(srcset=re.compile(link_name))
                    if found != []:
                        for pic in found:
                            if pic['srcset'] not in pics:
                                pics.append(pic['srcset'])

                pics = delete_small_pics(pics)

            if pics == []:
                time.sleep(3)

                imgs = soup.find_all('img', recursive=True)
                for img in imgs:
                    for attr in img.attrs.values():
                        if type(attr).__name__ != 'list':
                            if '.jpg' in attr or '.jpeg' in attr or '.png' in attr:
                                pics.append(attr)
                    # if img.has_attr('src'):
                    #     pics.append(img['src'])


                pics = delete_small_pics(pics)

            if pics == []:
                pic_flg = False
                print("с этой страницы картинки не распарсились")
            else:
                print(pics)
                pic_flg = True

        else:
            pic_flg = False
            print("нет смысла смотреть картинки")


        desc = None
        desc_flg = False
        if name_flg:
            #описание
            descriptions = soup.find_all(True, {'class' : re.compile('description')})
            if descriptions == []:
                descriptions = soup.find_all(True, {'id' : re.compile('description')})
            if descriptions == []:
                descriptions = soup.find_all('p')
            if descriptions != []:
                descriptions.sort(key = lambda s: len(s.text), reverse=True)
                desc = descriptions[0].string
                print(desc)
                desc_flg = True
            else:
                desc_flg = False



        if name_flg:
            #категория
            category = ''
            for keyword in keywords:
                if keyword in name:
                    if ('brush' in name or 'Brush' in name):
                        category = 'Brush'
                    else:
                        category = keyword.lower()
                    break
            if category == '':
                if desc != None:
                    for keyword in keywords:
                        if keyword in desc:
                            if('brush' in desc or 'Brush' in desc):
                                category = 'Brush'
                            else:
                                category = keyword.lower()
                            break
            if category == '':
                for keyword in keywords:
                    if page_title.string != None:
                        if keyword in page_title.string:
                            if desc != None:
                                if ('brush' in desc or 'Brush' in desc):
                                    category = 'Brush'
                                else:
                                    category = keyword.lower()
                            else:
                                category = keyword.lower()
                            break
            if category == '':
                category = 'other'
            if category == 'lip':
                category = 'lipstick'
            print(category)



        shades_flg=False
        #оттенки
        # if pic_flg and name_flg and price_flg:
        #     shades = []
        #     img_tag = soup.find_all('img')
        #     imgs = []
        #     for el in img_tag:
        #         if el.has_attr('src'):
        #             imgs.append(el['src'])
        #     shades = shades_from_list_of_urls(imgs)
        #
        #     if shades == []:
        #         results = soup.findAll(True, {"background-color": re.compile('#')})
        #         additional = soup.findAll(True, {"style": re.compile('background-color:#')})
        #         for el in additional:
        #             hex = re.match('(?<=background-color:#).{6}', el['style'])
        #             if hex != None:
        #                 shades.append(hex)
        #         for res in results:
        #             if res['background-color'] != None:
        #                 shades.append(res['background-color'])
        #
        #     if shades == []:
        #         els = soup.find_all('a', {'data-url':True})
        #         for el in els:
        #             shades.append(el['data-url'])
        #     if shades != []:
        #         shades_flg = True
        #         shade_colors = []
        #         for shade in shades:
        #             src = shade
        #             if src == '':
        #                 continue
        #             if src[0] == '/' and src[1] == '/':
        #                 src = prefix + src[2:]
        #             if src[0] == '/':
        #                 src = prefix + website + postfix + src
        #
        #             extens = src.split('?')[0][-4:]
        #             if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
        #                 continue
        #             if extens == 'jpeg':
        #                 extens = '.jpeg'
        #
        #             try:
        #                 img_data = requests.get(src).content
        #             except:
        #                 if src[:4] == 'http':
        #                     src_els = src.split('://')
        #                     src = src_els[0] + '://www.' + src_els[1]
        #                     img_data = requests.get(src).content
        #                 else:
        #                     src = prefix + website + postfix + '/' + src
        #                     img_data = requests.get(src).content
        #
        #             with open('aaa' + extens, 'wb') as handler:
        #                 handler.write(img_data)
        #
        #
        #             image = Image.open('aaa' + extens)
        #             shade = image.convert('RGB')
        #             colors = shade.getcolors(maxcolors=10000)
        #             colors.sort(key=operator.itemgetter(0), reverse=True)
        #             colors = colors[:3]
        #             for i in range(0, len(colors) - 1):
        #                 if colors[i][1][0] + colors[i][1][1] + colors[i][1][2] >= 750 \
        #                         or colors[i][1][0] + colors[i][1][1] + colors[i][1][2] <= 15:
        #                     del colors[i]
        #             if colors[0] != None:
        #                 shade_colors.append(colors[0])
        #         shades_flg = False
        #         print(shade_colors)


        color_flag = False
        if name_flg:
            if category == 'lipstick' and pics != []:
                src = pics[0]
                if src == '':
                    continue
                if src[0] == '/' and src[1] == '/':
                    src = prefix + src[2:]
                if src[0] == '/':
                    src = prefix + website + postfix + src

                extens = src.split('?')[0][-4:]
                if extens != '.png' and extens != '.jpg' and extens != 'jpeg':
                    continue
                if extens == 'jpeg':
                    extens = '.jpeg'

                try:
                    img_data = requests.get(src).content
                except:
                    if src[:4] == 'http':
                        src_els = src.split('://')
                        src = src_els[0] + '://www.' + src_els[1]
                        try:
                            img_data = requests.get(src).content
                        except:
                            continue
                    else:
                        src = prefix + website + postfix + '/' + src
                        img_data = requests.get(src).content

                with open('aaa' + extens, 'wb') as handler:
                    handler.write(img_data)

                try:
                    image = Image.open('aaa' + extens)
                    if(extens != '.jpg'):
                        rgb_im = image.convert('RGB')
                        rgb_im.save('aaa.jpg')
                    image = Image.open('aaa' + '.jpg')
                    image_np = load_image_into_numpy_array(image)
                    image_np_expanded = np.expand_dims(image_np, axis=0)
                    output_dict = run_inference_for_single_image(image_np_expanded, detection_graph)
                    box = output_dict['detection_boxes'][0]
                    print(box)
                    width, height = image.size
                    print(width)
                    print(height)

                    ymin = box[0] * height
                    xmin = box[1] * width
                    ymax = box[2] * height
                    xmax = box[3] * width

                    cropped = image.crop((xmin, ymin, xmax, ymax))
                    cropped = cropped.convert('RGB')
                    colors = cropped.getcolors(maxcolors=cropped.size[0]*cropped.size[1])
                    colors.sort(key=operator.itemgetter(0), reverse=True)
                    colors = colors[:3]
                    for i in range(0, len(colors) - 1):
                        if colors[i][1][0] + colors[i][1][1] + colors[i][1][2] >= 750 \
                                or colors[i][1][0] + colors[i][1][1] + colors[i][1][2] <= 15:
                            del colors[i]
                    if colors[0] != None:
                        color_flag = True
                        color = colors[0][1]
                except Exception as e:
                    print(e)



        if price_flg:
            price = price.replace('.', ",")
            price = price.replace(',,', ",")
            price = price.replace('.,', ",")
        company_name = company_name.replace('.', " ")
        company_name = company_name.replace('/', " ")
        if name != None:
            name = name.replace('.', " ")
            name = name.replace('/', " ")
            name = name.replace('%', " ")
            name = name.replace('®', "")
            name = name.replace('$', "")



        dict = {}
        if (name_flg and pic_flg and price_flg):

            try:
                old_price = ref.child('products').child(name + "Ф" + company_name + price).child('price').get()
            except:
                print("a")
            if (old_price != None):
                if (int(re.sub('[^0-9]', '', old_price)) > int(re.sub('[^0-9]', '', price))):
                    ref.child('sale').update({name + "Ф" + company_name + price: {'old': old_price, 'new': price}})

            try:
                old_product = ref.child('products').child(name + "Ф" + company_name + price).get()
                if (old_product == {}):
                    ref.child('new').update({name + "Ф" + company_name + price: 0})
            except:
                print("oops")

            dict['category'] = category
            dict['pics'] = pics
            dict['name'] = name
            dict['price'] = price
            dict['rating'] = '?'
            dict['numReviews'] = '0'
            if (desc_flg):
                dict['description'] = desc
            # if (shades_flg):
            #     dict['shades'] = shade_colors
            if (color_flag):
                dict['r'] = color[0]
                dict['g'] = color[1]
                dict['b'] = color[2]
            else:
                dict['r'] = 0
                dict['g'] = 0
                dict['b'] = 0

            elize = get_elize()
            if(elize == None):
                dict['elize'] = 'N/A'
            else:
                dict['elize'] = elize

            ilde = get_ildebote()
            if(ilde == None):
                dict['ildebote'] = 'N/A'
            else:
                dict['ildebote'] = ilde

            gold = get_goldenapple()
            if (gold == None):
                dict['goldenapple'] = 'N/A'
            else:
                dict['goldenapple'] = gold

            ref.child('products').update({name + "Ф" + company_name + price: dict})
            ref.child('companies').child(company_name).child(category).update({name + "Ф" + company_name + price: 0})
